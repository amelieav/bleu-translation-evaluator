"""

This Python script is designed for evaluating the quality of translations generated by a chatbot using the BLEU (Bilingual Evaluation Understudy) metric.

The script provides two main functions:

1. `evaluate_chatbot_responses(references, generated_texts)`: This function calculates BLEU scores for a list of reference texts and corresponding generated texts. It tokenizes the text, computes BLEU scores with smoothing for short sentences, and returns a list of individual BLEU scores.

2. `overall_evaluation(bleu_scores, threshold=0.5)`: This function takes a list of BLEU scores and an optional threshold value. 
It computes the average BLEU score and provides an overall evaluation based on the average score. The possible evaluations include "Excellent," "Good," "Fair," or "Poor," depending on the average BLEU score in comparison to the specified threshold. These are vague labels, feel free to adjust.


"""

import nltk
from nltk.translate.bleu_score import sentence_bleu  # calculate BLEU score
from nltk.translate.bleu_score import (
    SmoothingFunction,
)  # to not penalise short sentences as much


def evaluate_chatbot_responses(references, generated_texts):
    """
    Calculates BLEU scores for a list of reference texts and corresponding generated texts.

    Args:
        references (list): A list of reference texts.
        generated_texts (list): A list of generated texts.

    Returns:
        bleu_scores (list): A list of BLEU scores for each text specified.

    """
    bleu_scores = []
    for reference, generated_text in zip(references, generated_texts):
        reference_tokens = nltk.word_tokenize(reference)
        generated_tokens = nltk.word_tokenize(generated_text)

        # Calculate BLEU score with smoothing to prevent over penalisation of short sentences
        bleu_score = sentence_bleu(
            [reference_tokens],
            generated_tokens,
            smoothing_function=SmoothingFunction().method1,
        )
        bleu_scores.append(bleu_score)

    return bleu_scores


def overall_evaluation(bleu_scores, threshold=0.5):
    """
    Prints out a final evaluation based on the average BLEU score.

    Args:
        bleu_scores (list): A list of BLEU scores for each text specified.
        threshold (float): The threshold for the average BLEU score. Defaults to 0.5, feel free to adjust.

    Returns:
        overall (str): A string indicating the overall evaluation based on the average BLEU score.
        The possible evaluations include "Excellent," "Good," "Fair," or "Poor," depending on the average BLEU score in comparison to the specified threshold. These are vague labels, feel free to adjust.
    """
    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)

    if avg_bleu_score >= threshold:
        return "Excellent"
    elif avg_bleu_score >= 0.4:
        return "Good"
    elif avg_bleu_score >= 0.3:
        return "Fair"
    else:
        return "Poor"


# original text
references = [
    "The white wagtail (Motacilla alba) is a small bird found in Europe, Asia, North Africa, and occasionally Alaska. In Ireland and Great Britain, a darker subspecies called the pied wagtail (M. a. yarrellii) is common. There are several subspecies in total.",
]

# a "perfect" translation:

# La bergeronnette blanche (Motacilla alba) est un petit oiseau que l’on trouve en Europe, en Asie, en Afrique du Nord et parfois en Alaska. 
# En Irlande et en Grande-Bretagne, une sous-espèce plus foncée appelée la bergeronnette pied (M. a. yarrellii) est commune. 
# Il existe plusieurs sous-espèces au total.

# translated by chatbot
generated_texts = [
    "The white wagtail (Motacilla alba) is a small bird found in Europe, Asia, North Africa, and sometimes in Alaska. In Ireland and Great Britain, a darker subspecies called the pied wagtail (M. a. yarrellii) is common. There are several subspecies in total.",
]

bleu_scores = evaluate_chatbot_responses(references, generated_texts)
overall = overall_evaluation(bleu_scores)

print("Generated BLEU Scores:", bleu_scores)
print("Overall Evaluation:", overall)
